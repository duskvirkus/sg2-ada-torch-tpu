{
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 2.7.17 64-bit"
  },
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python 3.7.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#required: definitely edit these!\n",
    "dataset_path = '/content/drive/MyDrive/datasets/love-words-v1.zip'\n",
    "resume_from = '/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00017-love-words-v1-11gb-gpu-complex-gamma50-bgc-resumecustom/network-snapshot-000020.pkl'\n",
    "aug_strength = 0.0\n",
    "train_count = 0\n",
    "mirror_x = False\n",
    "mirror_y = False\n",
    "\n",
    "#optional: you might not need to edit these\n",
    "gamma_value = 50.0\n",
    "augs = 'bgc'\n",
    "config = '11gb-gpu-complex'\n",
    "snapshot_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\nUsage: train.py [OPTIONS]\n\n  Train a GAN using the techniques described in\n  the paper \"Training Generative Adversarial\n  Networks with Limited Data\".\n\n  Examples:\n\n  # Train with custom dataset using 1 GPU.\n  python train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1\n\n  # Train class-conditional CIFAR-10 using 2 GPUs.\n  python train.py --outdir=~/training-runs --data=~/datasets/cifar10.zip \\\n      --gpus=2 --cfg=cifar --cond=1\n\n  # Transfer learn MetFaces from FFHQ using 4 GPUs.\n  python train.py --outdir=~/training-runs --data=~/datasets/metfaces.zip \\\n      --gpus=4 --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n\n  # Reproduce original StyleGAN2 config F.\n  python train.py --outdir=~/training-runs --data=~/datasets/ffhq.zip \\\n      --gpus=8 --cfg=stylegan2 --mirror=1 --aug=noaug\n\n  Base configs (--cfg):\n    auto       Automatically select reasonable defaults based on resolution\n               and GPU count. Good starting point for new datasets.\n    stylegan2  Reproduce results for StyleGAN2 config F at 1024x1024.\n    paper256   Reproduce results for FFHQ and LSUN Cat at 256x256.\n    paper512   Reproduce results for BreCaHAD and AFHQ at 512x512.\n    paper1024  Reproduce results for MetFaces at 1024x1024.\n    cifar      Reproduce results for CIFAR-10 at 32x32.\n\n  Transfer learning source networks (--resume):\n    ffhq256        FFHQ trained at 256x256 resolution.\n    ffhq512        FFHQ trained at 512x512 resolution.\n    ffhq1024       FFHQ trained at 1024x1024 resolution.\n    celebahq256    CelebA-HQ trained at 256x256 resolution.\n    lsundog256     LSUN Dog trained at 256x256 resolution.\n    <PATH or URL>  Custom network pickle.\n\nOptions:\n  --outdir DIR                    Where to save\n                                  the results\n                                  [required]\n\n  --gpus INT                      Number of GPUs\n                                  to use [default:\n                                  1]\n\n  --snap INT                      Snapshot\n                                  interval\n                                  [default: 50\n                                  ticks]\n\n  --metrics LIST                  Comma-separated\n                                  list or \"none\"\n                                  [default:\n                                  fid50k_full]\n\n  --tpu BOOL                      Use TPUs instead\n                                  of GPUs.\n\n  --seed INT                      Random seed\n                                  [default: 0]\n\n  -n, --dry-run                   Print training\n                                  options and exit\n\n  --data PATH                     Training data\n                                  (directory or\n                                  zip)  [required]\n\n  --cond BOOL                     Train\n                                  conditional\n                                  model based on\n                                  dataset labels\n                                  [default: false]\n\n  --subset INT                    Train with only\n                                  N images\n                                  [default: all]\n\n  --mirror BOOL                   Enable dataset\n                                  x-flips\n                                  [default: false]\n\n  --mirrory BOOL                  Augment dataset\n                                  with y-flips\n                                  (default: false)\n\n  --cfg [auto|11gb-gpu|11gb-gpu-complex|24gb-gpu|24gb-gpu-complex|48gb-gpu|48gb-2gpu|stylegan2|paper256|paper512|paper1024|cifar|cifarbaseline|aydao]\n                                  Base config\n                                  [default: auto]\n\n  --lrate FLOAT                   Override\n                                  learning rate\n\n  --gamma FLOAT                   Override R1\n                                  gamma\n\n  --kimg INT                      Override\n                                  training\n                                  duration\n\n  --nkimg INT                     Override\n                                  starting count\n\n  --batch INT                     Override batch\n                                  size\n\n  --aug [noaug|ada|fixed]         Augmentation\n                                  mode [default:\n                                  ada]\n\n  --p FLOAT                       Augmentation\n                                  probability for\n                                  --aug=fixed\n\n  --target FLOAT                  ADA target value\n                                  for --aug=ada\n\n  --augpipe [blit|geom|color|filter|noise|cutout|bg|bgc|bgcf|bgcfn|bgcfnc]\n                                  Augmentation\n                                  pipeline\n                                  [default: bgc]\n\n  --initstrength FLOAT            Override ADA\n                                  strength at\n                                  start\n\n  --resume PKL                    Resume training\n                                  [default:\n                                  noresume]\n\n  --freezed INT                   Freeze-D\n                                  [default: 0\n                                  layers]\n\n  --fp32 BOOL                     Disable mixed-\n                                  precision\n                                  training\n\n  --nhwc BOOL                     Use NHWC memory\n                                  format with FP16\n\n  --nobench BOOL                  Disable cuDNN\n                                  benchmarking\n\n  --allow-tf32 BOOL               Allow PyTorch to\n                                  use TF32\n                                  internally\n\n  --workers INT                   Override number\n                                  of DataLoader\n                                  workers\n\n  --help                          Show this\n                                  message and\n                                  exit.\n"
     ]
    }
   ],
   "source": [
    "!python ../train.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Traceback (most recent call last):\n  File \"../train.py\", line 18, in <module>\n    import dnnlib\n  File \"/content/drive/MyDrive/colab-sg2-ada-torch-tpu-devel/sg2-ada-torch-tpu/dnnlib/__init__.py\", line 9, in <module>\n    from .util import EasyDict, make_cache_dir_path\n  File \"/content/drive/MyDrive/colab-sg2-ada-torch-tpu-devel/sg2-ada-torch-tpu/dnnlib/util.py\", line 43\n    def __getattr__(self, name: str) -> Any:\n                              ^\nSyntaxError: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!python ../train.py --tpu=True --cfg=$config --metrics=None --outdir=./results --data=$dataset_path --snap=$snapshot_count --resume=$resume_from --augpipe=$augs --initstrength=$aug_strength --gamma=$gamma_value --mirror=$mirror_x --mirrory=False --nkimg=$train_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}